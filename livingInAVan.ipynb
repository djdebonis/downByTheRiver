{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as van"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ls of ls as dataset\n",
    "salientWords = van.readData(\"salientWords.txt\")\n",
    "\n",
    "# check to see if ls is same len after reading in data from doc\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document is not same length. extra item was thrown in. pop this item to remove it from list\n",
    "salientWords.pop(3101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check len again\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code creates a dictionary called documentFrequency which determines the total use of \n",
    "# individual/unique words across *all* documents\n",
    "documentIndex = {} \n",
    "for sentIndex, sent in enumerate(salientWords):\n",
    "    for word_index, word in enumerate(sent):\n",
    "        try:\n",
    "            # checks to see if there is an existing index and, if there is & the value of the \n",
    "            # sentence doesn't already exist, it adds the value\n",
    "            documentIndex[word].add(sentIndex)\n",
    "        except:\n",
    "            # if there is not an existing index, it creates one and *then* adds the value\n",
    "            documentIndex[word] = {sentIndex}\n",
    "\n",
    "# documentIndex dictionary is indexed by the unique word. each entry is the sentence (document) #\n",
    "\n",
    "documentFrequency = {}\n",
    "\n",
    "for dictIndex, dictElement in enumerate(documentIndex):\n",
    "    documentFrequency[dictElement] = len(documentIndex[dictElement]) # add the number\n",
    "    # of documents that contain this index (word) to the new dict.\n",
    "    \n",
    "# documentFrequency is a dictionary containing 'unique_word': howManySentencesItOccursIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3101 total documents/sentences in the book\n",
      "After the data was cleaned and processed there were 30970 total words remaining\n"
     ]
    }
   ],
   "source": [
    "# calculate total documents/sent\n",
    "totalDocuments = len(salientWords)\n",
    "print(\"There are %d total documents/sentences in the book\"%(totalDocuments))\n",
    "\n",
    "# calculate total words\n",
    "wordLen = 0\n",
    "for i,e in enumerate(salientWords):\n",
    "    wordLen += len(e)\n",
    "print(\"After the data was cleaned and processed there were %d total words remaining\"%(wordLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "termFrequencyLs = [] # create ls for term frequency (TF)\n",
    "inverseDocFrequencyLs = [] # create ls for inverse document frequency (IDF)\n",
    "tfidfLs = [] # create ls for term frequency-inverse document frequency (TF-IDF)\n",
    "for sent_index, sent in enumerate(salientWords): # loop through each document\n",
    "    # create a sep list for each document's stats\n",
    "    tempTermFrequency = [] \n",
    "    tempInverseDocFrequency = []\n",
    "    tempTfidf = []\n",
    "    docLength = len(sent) # length of the current document (how many words in this sentence)\n",
    "    counts = Counter(sent) # create a counter for unique values and their frequency per document\n",
    "    for word_index, word in enumerate(sent): # iterate through each word in the current document\n",
    "        frequencyInDoc = counts[word] # frequency of current word\n",
    "        termFrequency = frequencyInDoc / docLength # term frequency for this word in this doc\n",
    "        tempTermFrequency.append(termFrequency) \n",
    "        \n",
    "        \n",
    "        inverseDocFrequency = math.log((totalDocuments/(documentFrequency[word]))) # inverse doc\n",
    "        # frequency for this word in this doc\n",
    "        tempInverseDocFrequency.append(inverseDocFrequency)\n",
    "        \n",
    "        tfidf = (termFrequency * inverseDocFrequency) #tfidf for this word in this doc\n",
    "        tempTfidf.append(tfidf)\n",
    "    \n",
    "    # append the list of the current document to the big-boy list\n",
    "    termFrequencyLs.append(tempTermFrequency)\n",
    "    inverseDocFrequencyLs.append(tempInverseDocFrequency)\n",
    "    tfidfLs.append(tempTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interesting. The following prints the top three TFIDF value for each document (i.e. sentence)\n",
    "## SUPER INTERESTING. THE CODE BELOW GIVES A LOT OF MEANING IN THREE WORDS PER SENTENCE.\n",
    "maximumIndexLs = []\n",
    "\n",
    "for index, ls in enumerate(tfidfLs):\n",
    "    #maximumIndex = np.argmax(ls)\n",
    "    #maximumIndexLs.append(maximumIndex)\n",
    "    #topThree = []\n",
    "    n = 3\n",
    "    topThreeIndeces = sorted(range(len(ls)), key = lambda sub: ls[sub])[-n:] # some crazy ass lambda function I found that grabs the indecies of the\n",
    "    # three largest items. if someone can explain to me how this is working that would be lovely. all i know is that it works!\n",
    "    # print(topThreeIndeces)\n",
    "    maximumIndexLs.append(topThreeIndeces)\n",
    "        \n",
    "sentenceNum = []\n",
    "wordOne = []\n",
    "wordTwo = []\n",
    "wordThree = []\n",
    "if len(maximumIndexLs) == len(salientWords): # check to make sure we can iterate over these together\n",
    "    for index, doc in enumerate(salientWords):\n",
    "        #print(\"Sentence # %d: \"%(index))\n",
    "        indexLs = maximumIndexLs[index]\n",
    "        if len(indexLs) ==3: \n",
    "            sentenceNum.append((index + 1))\n",
    "            for j in range(3):\n",
    "                tempWord = doc[(indexLs[j])]\n",
    "                #print(\"Word %d: %s\"%((j+1), tempWord))\n",
    "                if j == 0:\n",
    "                    wordOne.append(tempWord)\n",
    "                elif j == 1:\n",
    "                    wordTwo.append(tempWord)\n",
    "                elif j == 2:\n",
    "                    wordThree.append(tempWord)\n",
    "        else:\n",
    "            #print(\"Wrong number of maximums\")\n",
    "            someValue = 0\n",
    "else:\n",
    "    print(\"Not same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNum</th>\n",
       "      <th>wordOne</th>\n",
       "      <th>wordTwo</th>\n",
       "      <th>wordThree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>starte</td>\n",
       "      <td>day</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>guess</td>\n",
       "      <td>catching</td>\n",
       "      <td>beings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>please</td>\n",
       "      <td>kurt</td>\n",
       "      <td>vonnegut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>story</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>3096</td>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>compromise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>3097</td>\n",
       "      <td>ostracized</td>\n",
       "      <td>majority</td>\n",
       "      <td>compassion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>3098</td>\n",
       "      <td>receive</td>\n",
       "      <td>unshaved</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>3100</td>\n",
       "      <td>lonesome</td>\n",
       "      <td>city</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>3101</td>\n",
       "      <td>understood</td>\n",
       "      <td>stayed</td>\n",
       "      <td>tank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentenceNum     wordOne   wordTwo   wordThree\n",
       "0               1      denver    denver      denver\n",
       "1               2      starte       day         day\n",
       "2               3       guess  catching      beings\n",
       "3               4      please      kurt    vonnegut\n",
       "4               5   pneumonia     story       story\n",
       "...           ...         ...       ...         ...\n",
       "2936         3096       could     could  compromise\n",
       "2937         3097  ostracized  majority  compassion\n",
       "2938         3098     receive  unshaved    brushing\n",
       "2939         3100    lonesome      city        city\n",
       "2940         3101  understood    stayed        tank\n",
       "\n",
       "[2941 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe with the top three words for each sentence\n",
    "topThreeWords = pd.DataFrame(zip(sentenceNum,wordOne,wordTwo,wordThree), columns = [\"sentenceNum\", \"wordOne\", \"wordTwo\", \"wordThree\"])\n",
    "topThreeWords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2109, 1015, 772, 2602, 2415, 2748, 1894, 373, 1290, 876]\n"
     ]
    }
   ],
   "source": [
    "# create a set of random pointers to filter out the word sets\n",
    "random.seed(120)\n",
    "pointers = []\n",
    "\n",
    "for i in range(10):\n",
    "    ptr = random.randint(0, len(sentenceNum))\n",
    "    pointers.append(ptr)\n",
    "    \n",
    "print(pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNum</th>\n",
       "      <th>wordOne</th>\n",
       "      <th>wordTwo</th>\n",
       "      <th>wordThree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>1367</td>\n",
       "      <td>outdoorsey</td>\n",
       "      <td>carharrts</td>\n",
       "      <td>khuls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1368</td>\n",
       "      <td>instead</td>\n",
       "      <td>bozeman</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1369</td>\n",
       "      <td>david</td>\n",
       "      <td>office</td>\n",
       "      <td>inform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentenceNum     wordOne    wordTwo wordThree\n",
       "1290         1367  outdoorsey  carharrts     khuls\n",
       "1291         1368     instead    bozeman      vibe\n",
       "1292         1369       david     office    inform"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the df according to the pointer and the length of sentences. append each df to a list\n",
    "length = 3\n",
    "sets = []\n",
    "\n",
    "for index, ptr in enumerate(pointers):\n",
    "    cap = ptr + length # cap is the top index in DataFrame[ptr:cap]\n",
    "    tempDf = topThreeWords[ptr:cap]\n",
    "    sets.append(tempDf)\n",
    "    filePath = \"randomSamples/randomSet\" + str(index) + \".csv\"\n",
    "    tempDf.to_csv(filePath)\n",
    "     \n",
    "sets[8] # one is interestin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOriginalSentences(lsOfSent, lsOfPointers, lengthOfSet):\n",
    "    \n",
    "    sets = []\n",
    "\n",
    "    for index, ptr in enumerate(lsofPointers):\n",
    "        cap = ptr + length # cap is the top index in DataFrame[ptr:cap]\n",
    "        subSet = lsOfSent[ptr:cap]\n",
    "        string = \"\\n\".join(subSet)\n",
    "        print(string)\n",
    "        print(\"\")\n",
    "        #sets.append(subSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentFrequency[\"van\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueDf(uniqueArr):\n",
    "    unique = pd.DataFrame(uniqueArr, columns=['token', 'indexFirstTokenOccur','frequency']) # convert to dataframe for easier wrangling\n",
    "    unique.frequency = unique.frequency.astype(int) # before this line was added, the count/frequency had at some point been converted to a string and so\n",
    "    # the sort values function was sorting it based upon the first value in a string (e.g. 1 comes before 9, so 1203 is smaller tahn 99)\n",
    "    unique = unique.sort_values(by='frequency', ascending = False)\n",
    "    #pring the head\n",
    "    \n",
    "    return(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueArr(lsOfAllWords):\n",
    "    \n",
    "    uniqueTokens, uniqueFirstOccurance, uniqueTokenCounts = np.unique(lsOfAllWords, return_index=True, return_counts=True) # how many unique words did I use in the writing of \n",
    "    # this book? what were the counts?\n",
    "    uniqueArr = np.asarray((uniqueTokens,uniqueFirstOccurance,uniqueTokenCounts)).T # turns tuple into ndarray then .T transposes it over its axis\n",
    "    \n",
    "    return(uniqueArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# although we already put this through a dict above, I would rather use lists\n",
    "allSalientWords = []\n",
    "\n",
    "for sentIndex, sent in enumerate(salientWords):\n",
    "    for wordIndex, word in enumerate(sent):\n",
    "        allSalientWords.append(word)\n",
    "\n",
    "len(allSalientWords)\n",
    "(\"van\" in allSalientWords) # so the word van is in salient words, so it got lost somewhere in the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "jackeroo = 0\n",
    "for i, e in enumerate(allSalientWords):\n",
    "    if e == \"van\":\n",
    "        jackeroo += 1\n",
    "        \n",
    "print(jackeroo) # so we have the same len of van instances as the other doc, so we know\n",
    "# SOMETHING BREAKS AFTER THIS CELL\n",
    "# probably has to do with the damn pos tag shit lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>indexFirstTokenOccur</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3211</th>\n",
       "      <td>like</td>\n",
       "      <td>376</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5866</th>\n",
       "      <td>van</td>\n",
       "      <td>280</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>back</td>\n",
       "      <td>262</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>said</td>\n",
       "      <td>29</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5592</th>\n",
       "      <td>time</td>\n",
       "      <td>64</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token indexFirstTokenOccur  frequency\n",
       "3211  like                  376        211\n",
       "5866   van                  280        163\n",
       "429   back                  262        159\n",
       "4668  said                   29        158\n",
       "5592  time                   64        157"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSalientWordsArr = uniqueArr(allSalientWords)\n",
    "allSalientWordsDf = uniqueDf(allSalientWordsArr)\n",
    "allSalientWordsDf.head() # so if i just let it chill like this it's all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "altAxis = allSalientWordsArr.transpose()\n",
    "\n",
    "\n",
    "posTuples = nltk.pos_tag(altAxis[0])\n",
    "posLs = []\n",
    "\n",
    "for i in range(len(posTuples)):\n",
    "    tup = posTuples[i]\n",
    "    pos = tup[1]\n",
    "    posLs.append(pos)\n",
    "#posLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6190, 3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSalientWordsArr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['CD', 'CD', 'CD', ..., 'VBD', 'CD', 'NN']], dtype='<U4')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posArr = np.array([posLs])\n",
    "posArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(allSalientWordsArr) == len(posLs):\n",
    "    final = np.hstack([allSalientWordsArr, posArr.T])\n",
    "else:\n",
    "    print(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3am', '17796', '1', 'CD'],\n",
       "       ['4', '580', '12', 'CD'],\n",
       "       ['40', '19730', '4', 'CD'],\n",
       "       ['400lb', '20118', '1', 'CD'],\n",
       "       ['45', '2741', '1', 'CD'],\n",
       "       ['45am', '30804', '1', 'CD'],\n",
       "       ['4am', '4270', '3', 'CD'],\n",
       "       ['4runner', '2414', '3', 'CD'],\n",
       "       ['4wd', '2386', '1', 'CD'],\n",
       "       ['4x4s', '22880', '1', 'CD'],\n",
       "       ['5', '2454', '9', 'CD'],\n",
       "       ['50', '212', '9', 'CD'],\n",
       "       ['500', '15703', '4', 'CD'],\n",
       "       ['50th', '7088', '2', 'JJ'],\n",
       "       ['5200', '215', '2', 'CD'],\n",
       "       ['55', '9680', '3', 'CD'],\n",
       "       ['5am', '28287', '1', 'CD'],\n",
       "       ['5ers', '18782', '2', 'NNS'],\n",
       "       ['5th', '2840', '1', 'CD'],\n",
       "       ['6', '4971', '5', 'CD'],\n",
       "       ['60', '6031', '3', 'CD'],\n",
       "       ['60lbs', '13880', '1', 'CD'],\n",
       "       ['625', '26415', '1', 'CD'],\n",
       "       ['65', '4867', '1', 'CD'],\n",
       "       ['6am', '17421', '1', 'CD'],\n",
       "       ['6inches', '30103', '1', 'CD'],\n",
       "       ['7', '20058', '2', 'CD'],\n",
       "       ['70', '654', '8', 'CD'],\n",
       "       ['710', '6024', '2', 'CD'],\n",
       "       ['78', '17309', '1', 'CD'],\n",
       "       ['7am', '16556', '1', 'CD'],\n",
       "       ['7th', '174', '2', 'CD'],\n",
       "       ['8', '25537', '1', 'CD'],\n",
       "       ['80', '17310', '2', 'CD'],\n",
       "       ['8am', '1611', '1', 'CD'],\n",
       "       ['9', '12264', '6', 'CD'],\n",
       "       ['90', '444', '6', 'CD'],\n",
       "       ['96oz', '14970', '1', 'CD'],\n",
       "       ['abandon', '10828', '2', 'NNS'],\n",
       "       ['abandoned', '7844', '2', 'VBN'],\n",
       "       ['ability', '19863', '2', 'NN'],\n",
       "       ['able', '3929', '14', 'JJ'],\n",
       "       ['ablilities', '10414', '1', 'NNS'],\n",
       "       ['absence', '30515', '1', 'IN'],\n",
       "       ['absolute', '1008', '4', 'NN'],\n",
       "       ['absolutely', '4180', '10', 'RB'],\n",
       "       ['absorb', '2117', '1', 'JJ'],\n",
       "       ['absorbed', '24161', '2', 'VBN'],\n",
       "       ['absurd', '15550', '1', 'JJ'],\n",
       "       ['abyss', '1217', '5', 'NN'],\n",
       "       ['accelerated', '4809', '1', 'VBD'],\n",
       "       ['accelerating', '4899', '2', 'VBG'],\n",
       "       ['accent', '1575', '1', 'NN'],\n",
       "       ['accents', '9083', '1', 'NNS'],\n",
       "       ['accept', '20549', '1', 'VBP'],\n",
       "       ['accepted', '3136', '3', 'VBN'],\n",
       "       ['accidentally', '7852', '1', 'RB'],\n",
       "       ['accomplish', '14078', '2', 'JJ'],\n",
       "       ['accordance', '30581', '1', 'NN'],\n",
       "       ['according', '1851', '4', 'VBG'],\n",
       "       ['accounted', '23823', '2', 'VBD'],\n",
       "       ['accurately', '1014', '1', 'RB'],\n",
       "       ['acedemia', '20484', '1', 'JJ'],\n",
       "       ['ache', '10045', '1', 'NN'],\n",
       "       ['ached', '10314', '1', 'VBD'],\n",
       "       ['acheivement', '19911', '1', 'JJ'],\n",
       "       ['achievement', '4625', '1', 'NN'],\n",
       "       ['aching', '8783', '2', 'VBG'],\n",
       "       ['acid', '3321', '4', 'JJ'],\n",
       "       ['acorns', '17136', '2', 'NNS'],\n",
       "       ['acorss', '17352', '1', 'JJ'],\n",
       "       ['acres', '15506', '3', 'NNS'],\n",
       "       ['across', '163', '109', 'IN'],\n",
       "       ['act', '908', '4', 'NN'],\n",
       "       ['acting', '7723', '2', 'VBG'],\n",
       "       ['actingcompleting', '24189', '2', 'VBG'],\n",
       "       ['action', '24195', '3', 'NN'],\n",
       "       ['activities', '26158', '1', 'NNS'],\n",
       "       ['activity', '15143', '1', 'NN'],\n",
       "       ['actor', '4189', '2', 'NN'],\n",
       "       ['actors', '5585', '1', 'NNS'],\n",
       "       ['actual', '26364', '1', 'JJ'],\n",
       "       ['actually', '154', '20', 'RB'],\n",
       "       ['ad', '30152', '1', 'NN'],\n",
       "       ['adam', '6800', '1', 'NN'],\n",
       "       ['adapted', '11040', '1', 'VBD'],\n",
       "       ['added', '22586', '1', 'JJ'],\n",
       "       ['adderandack', '16626', '1', 'NN'],\n",
       "       ['addie', '16764', '1', 'NN'],\n",
       "       ['address', '6868', '6', 'NN'],\n",
       "       ['addressed', '21479', '1', 'VBD'],\n",
       "       ['adjacent', '14358', '1', 'JJ'],\n",
       "       ['adjective', '15767', '1', 'JJ'],\n",
       "       ['adjust', '17903', '2', 'NN'],\n",
       "       ['admirable', '17307', '1', 'JJ'],\n",
       "       ['admire', '17453', '1', 'NN'],\n",
       "       ['adolecent', '147', '2', 'NN'],\n",
       "       ['adore', '27724', '1', 'RB'],\n",
       "       ['adrenaline', '2979', '4', 'JJ'],\n",
       "       ['adventure', '6219', '17', 'NN'],\n",
       "       ['adventures', '146', '11', 'VBZ'],\n",
       "       ['advertisement', '5418', '1', 'JJ'],\n",
       "       ['advice', '6727', '3', 'NN'],\n",
       "       ['aerodynamic', '452', '2', 'JJ'],\n",
       "       ['afford', '319', '2', 'JJ'],\n",
       "       ['african', '18872', '1', 'JJ'],\n",
       "       ['aftermarket', '23536', '1', 'NN'],\n",
       "       ['afternoon', '2521', '30', 'NN'],\n",
       "       ['afternoons', '28584', '1', 'NNS'],\n",
       "       ['afterthought', '8503', '1', 'VBD'],\n",
       "       ['againshakes', '23678', '1', 'NNS'],\n",
       "       ['age', '238', '7', 'NN'],\n",
       "       ['agency', '30153', '1', 'NN'],\n",
       "       ['aggressively', '30019', '1', 'RB'],\n",
       "       ['aging', '14492', '2', 'VBG'],\n",
       "       ['ago', '5122', '5', 'RB'],\n",
       "       ['agreed', '4432', '3', 'VBN'],\n",
       "       ['agreement', '6950', '1', 'NN'],\n",
       "       ['agressively', '4898', '1', 'RB'],\n",
       "       ['agriculture', '7573', '2', 'JJ'],\n",
       "       ['aguy', '20743', '1', 'NN'],\n",
       "       ['ah', '13307', '4', 'NN'],\n",
       "       ['ahead', '5800', '3', 'RB'],\n",
       "       ['ahold', '9009', '1', 'JJ'],\n",
       "       ['aid', '364', '2', 'NN'],\n",
       "       ['aimlessly', '4238', '6', 'RB'],\n",
       "       ['aims', '24304', '2', 'VBZ'],\n",
       "       ['air', '693', '60', 'NN'],\n",
       "       ['airplanes', '5413', '2', 'NNS'],\n",
       "       ['aisle', '7948', '3', 'VBP'],\n",
       "       ['aisles', '8128', '1', 'NNS'],\n",
       "       ['aka', '14725', '3', 'VBP'],\n",
       "       ['alarm', '8519', '4', 'JJ'],\n",
       "       ['alber', '20415', '1', 'NN'],\n",
       "       ['albert', '24992', '1', 'JJ'],\n",
       "       ['alberta', '12273', '6', 'NN'],\n",
       "       ['alchemy', '8656', '1', 'NN'],\n",
       "       ['alcohol', '12204', '6', 'NN'],\n",
       "       ['alcoholic', '15709', '1', 'JJ'],\n",
       "       ['alcoholics', '15712', '1', 'NNS']], dtype='<U21')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[60:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 3, 1, 1])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6190"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allSalientWordsArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
