{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(infilePathAndName):\n",
    "    \"\"\"\n",
    "    Takes in the preprocessed/precleaned wordset that I wrote and returns a list of lists\n",
    "    containing the words tokenized into sentences (lists) and words (elements of lists)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(infilePathAndName, \"r\") as f:\n",
    "        string = f.read()\n",
    "        ls1 = string.split(\"\\n\")\n",
    "        finalLs = []\n",
    "        for index, string in enumerate(ls1):\n",
    "            subLs = string.split(\" \")\n",
    "            finalLs.append(subLs)\n",
    "            \n",
    "    return(finalLs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ls of ls as dataset\n",
    "salientWords = readData(\"salientWords.txt\")\n",
    "\n",
    "# check to see if ls is same len after reading in data from doc\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document is not same length. extra item was thrown in. pop this item to remove it from list\n",
    "salientWords.pop(3101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check len again\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code creates a dictionary called documentFrequency which determines the total use of \n",
    "# individual/unique words across *all* documents\n",
    "documentIndex = {} \n",
    "for sent_index, sent in enumerate(salientWords):\n",
    "    for word_index, word in enumerate(sent):\n",
    "        try:\n",
    "            # checks to see if there is an existing index and, if there is & the value of the \n",
    "            # sentence doesn't already exist, it adds the value\n",
    "            documentIndex[word].add(sent_index)\n",
    "        except:\n",
    "            # if there is not an existing index, it creates one and *then* adds the value\n",
    "            documentIndex[word] = {sent_index}\n",
    "\n",
    "# documentIndex dictionary is indexed by the unique word. each entry is the sentence (document) #\n",
    "\n",
    "documentFrequency = {}\n",
    "\n",
    "for dictIndex, dictElement in enumerate(documentIndex):\n",
    "    documentFrequency[dictElement] = len(documentIndex[dictElement]) # add the number\n",
    "    # of documents that contain this index (word) to the new dict.\n",
    "    \n",
    "# documentFrequency is a dictionary containing 'unique_word': howManySentencesItOccursIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3101 total documents/sentences in the book\n",
      "After the data was cleaned and processed there were 30970 total words remaining\n"
     ]
    }
   ],
   "source": [
    "# calculate total documents/sent\n",
    "totalDocuments = len(salientWords)\n",
    "print(\"There are %d total documents/sentences in the book\"%(totalDocuments))\n",
    "\n",
    "# calculate total words\n",
    "wordLen = 0\n",
    "for i,e in enumerate(salientWords):\n",
    "    wordLen += len(e)\n",
    "print(\"After the data was cleaned and processed there were %d total words remaining\"%(wordLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "termFrequencyLs = [] # create ls for term frequency (TF)\n",
    "inverseDocFrequencyLs = [] # create ls for inverse document frequency (IDF)\n",
    "tfidfLs = [] # create ls for term frequency-inverse document frequency (TF-IDF)\n",
    "for sent_index, sent in enumerate(salientWords): # loop through each document\n",
    "    # create a sep list for each document's stats\n",
    "    tempTermFrequency = [] \n",
    "    tempInverseDocFrequency = []\n",
    "    tempTfidf = []\n",
    "    docLength = len(sent) # length of the current document (how many words in this sentence)\n",
    "    counts = Counter(sent) # create a counter for unique values and their frequency per document\n",
    "    for word_index, word in enumerate(sent): # iterate through each word in the current document\n",
    "        frequencyInDoc = counts[word] # frequency of current word\n",
    "        termFrequency = frequencyInDoc / docLength # term frequency for this word in this doc\n",
    "        tempTermFrequency.append(termFrequency) \n",
    "        \n",
    "        \n",
    "        inverseDocFrequency = math.log((totalDocuments/(documentFrequency[word]))) # inverse doc\n",
    "        # frequency for this word in this doc\n",
    "        tempInverseDocFrequency.append(inverseDocFrequency)\n",
    "        \n",
    "        tfidf = (termFrequency * inverseDocFrequency) #tfidf for this word in this doc\n",
    "        tempTfidf.append(tfidf)\n",
    "    \n",
    "    # append the list of the current document to the big-boy list\n",
    "    termFrequencyLs.append(tempTermFrequency)\n",
    "    inverseDocFrequencyLs.append(tempInverseDocFrequency)\n",
    "    tfidfLs.append(tempTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interesting. The following prints the top three TFIDF value for each document (i.e. sentence)\n",
    "## SUPER INTERESTING. THE CODE BELOW GIVES A LOT OF MEANING IN THREE WORDS PER SENTENCE.\n",
    "maximumIndexLs = []\n",
    "\n",
    "for index, ls in enumerate(tfidfLs):\n",
    "    #maximumIndex = np.argmax(ls)\n",
    "    #maximumIndexLs.append(maximumIndex)\n",
    "    #topThree = []\n",
    "    n = 3\n",
    "    topThreeIndeces = sorted(range(len(ls)), key = lambda sub: ls[sub])[-n:] # some crazy ass lambda function I found that grabs the indecies of the\n",
    "    # three largest items. if someone can explain to me how this is working that would be lovely. all i know is that it works!\n",
    "    # print(topThreeIndeces)\n",
    "    maximumIndexLs.append(topThreeIndeces)\n",
    "        \n",
    "sentenceNum = []\n",
    "wordOne = []\n",
    "wordTwo = []\n",
    "wordThree = []\n",
    "if len(maximumIndexLs) == len(salientWords): # check to make sure we can iterate over these together\n",
    "    for index, doc in enumerate(salientWords):\n",
    "        #print(\"Sentence # %d: \"%(index))\n",
    "        indexLs = maximumIndexLs[index]\n",
    "        if len(indexLs) ==3: \n",
    "            sentenceNum.append((index + 1))\n",
    "            for j in range(3):\n",
    "                tempWord = doc[(indexLs[j])]\n",
    "                #print(\"Word %d: %s\"%((j+1), tempWord))\n",
    "                if j == 0:\n",
    "                    wordOne.append(tempWord)\n",
    "                elif j == 1:\n",
    "                    wordTwo.append(tempWord)\n",
    "                elif j == 2:\n",
    "                    wordThree.append(tempWord)\n",
    "        else:\n",
    "            #print(\"Wrong number of maximums\")\n",
    "            someValue = 0\n",
    "else:\n",
    "    print(\"Not same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "topThreeWords = pd.DataFrame(zip(sentenceNum,wordOne,wordTwo,wordThree), columns = [\"sentenceNum\", \"wordOne\", \"wordTwo\", \"wordThree\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNum</th>\n",
       "      <th>wordOne</th>\n",
       "      <th>wordTwo</th>\n",
       "      <th>wordThree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>starte</td>\n",
       "      <td>day</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>guess</td>\n",
       "      <td>catching</td>\n",
       "      <td>beings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>please</td>\n",
       "      <td>kurt</td>\n",
       "      <td>vonnegut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>story</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>3096</td>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>compromise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>3097</td>\n",
       "      <td>ostracized</td>\n",
       "      <td>majority</td>\n",
       "      <td>compassion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>3098</td>\n",
       "      <td>receive</td>\n",
       "      <td>unshaved</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>3100</td>\n",
       "      <td>lonesome</td>\n",
       "      <td>city</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>3101</td>\n",
       "      <td>understood</td>\n",
       "      <td>stayed</td>\n",
       "      <td>tank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentenceNum     wordOne   wordTwo   wordThree\n",
       "0               1      denver    denver      denver\n",
       "1               2      starte       day         day\n",
       "2               3       guess  catching      beings\n",
       "3               4      please      kurt    vonnegut\n",
       "4               5   pneumonia     story       story\n",
       "...           ...         ...       ...         ...\n",
       "2936         3096       could     could  compromise\n",
       "2937         3097  ostracized  majority  compassion\n",
       "2938         3098     receive  unshaved    brushing\n",
       "2939         3100    lonesome      city        city\n",
       "2940         3101  understood    stayed        tank\n",
       "\n",
       "[2941 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
