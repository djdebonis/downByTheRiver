{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions as van"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the ls of ls as dataset\n",
    "salientWords = van.readData(\"salientWords.txt\")\n",
    "\n",
    "# check to see if ls is same len after reading in data from doc\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document is not same length. extra item was thrown in. pop this item to remove it from list\n",
    "salientWords.pop(3101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check len again\n",
    "len(salientWords) == 3101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code creates a dictionary called documentFrequency which determines the total use of \n",
    "# individual/unique words across *all* documents\n",
    "documentIndex = {} \n",
    "for sentIndex, sent in enumerate(salientWords):\n",
    "    for word_index, word in enumerate(sent):\n",
    "        try:\n",
    "            # checks to see if there is an existing index and, if there is & the value of the \n",
    "            # sentence doesn't already exist, it adds the value\n",
    "            documentIndex[word].add(sentIndex)\n",
    "        except:\n",
    "            # if there is not an existing index, it creates one and *then* adds the value\n",
    "            documentIndex[word] = {sentIndex}\n",
    "\n",
    "# documentIndex dictionary is indexed by the unique word. each entry is the sentence (document) #\n",
    "\n",
    "documentFrequency = {}\n",
    "\n",
    "for dictIndex, dictElement in enumerate(documentIndex):\n",
    "    documentFrequency[dictElement] = len(documentIndex[dictElement]) # add the number\n",
    "    # of documents that contain this index (word) to the new dict.\n",
    "    \n",
    "# documentFrequency is a dictionary containing 'unique_word': howManySentencesItOccursIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3101 total documents/sentences in the book\n",
      "After the data was cleaned and processed there were 30970 total words remaining\n"
     ]
    }
   ],
   "source": [
    "# calculate total documents/sent\n",
    "totalDocuments = len(salientWords)\n",
    "print(\"There are %d total documents/sentences in the book\"%(totalDocuments))\n",
    "\n",
    "# calculate total words\n",
    "wordLen = 0\n",
    "for i,e in enumerate(salientWords):\n",
    "    wordLen += len(e)\n",
    "print(\"After the data was cleaned and processed there were %d total words remaining\"%(wordLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "termFrequencyLs = [] # create ls for term frequency (TF)\n",
    "inverseDocFrequencyLs = [] # create ls for inverse document frequency (IDF)\n",
    "tfidfLs = [] # create ls for term frequency-inverse document frequency (TF-IDF)\n",
    "for sent_index, sent in enumerate(salientWords): # loop through each document\n",
    "    # create a sep list for each document's stats\n",
    "    tempTermFrequency = [] \n",
    "    tempInverseDocFrequency = []\n",
    "    tempTfidf = []\n",
    "    docLength = len(sent) # length of the current document (how many words in this sentence)\n",
    "    counts = Counter(sent) # create a counter for unique values and their frequency per document\n",
    "    for word_index, word in enumerate(sent): # iterate through each word in the current document\n",
    "        frequencyInDoc = counts[word] # frequency of current word\n",
    "        termFrequency = frequencyInDoc / docLength # term frequency for this word in this doc\n",
    "        tempTermFrequency.append(termFrequency) \n",
    "        \n",
    "        \n",
    "        inverseDocFrequency = math.log((totalDocuments/(documentFrequency[word]))) # inverse doc\n",
    "        # frequency for this word in this doc\n",
    "        tempInverseDocFrequency.append(inverseDocFrequency)\n",
    "        \n",
    "        tfidf = (termFrequency * inverseDocFrequency) #tfidf for this word in this doc\n",
    "        tempTfidf.append(tfidf)\n",
    "    \n",
    "    # append the list of the current document to the big-boy list\n",
    "    termFrequencyLs.append(tempTermFrequency)\n",
    "    inverseDocFrequencyLs.append(tempInverseDocFrequency)\n",
    "    tfidfLs.append(tempTfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interesting. The following prints the top three TFIDF value for each document (i.e. sentence)\n",
    "## SUPER INTERESTING. THE CODE BELOW GIVES A LOT OF MEANING IN THREE WORDS PER SENTENCE.\n",
    "maximumIndexLs = []\n",
    "\n",
    "for index, ls in enumerate(tfidfLs):\n",
    "    #maximumIndex = np.argmax(ls)\n",
    "    #maximumIndexLs.append(maximumIndex)\n",
    "    #topThree = []\n",
    "    n = 3\n",
    "    topThreeIndeces = sorted(range(len(ls)), key = lambda sub: ls[sub])[-n:] # some crazy ass lambda function I found that grabs the indecies of the\n",
    "    # three largest items. if someone can explain to me how this is working that would be lovely. all i know is that it works!\n",
    "    # print(topThreeIndeces)\n",
    "    maximumIndexLs.append(topThreeIndeces)\n",
    "        \n",
    "sentenceNum = []\n",
    "wordOne = []\n",
    "wordTwo = []\n",
    "wordThree = []\n",
    "if len(maximumIndexLs) == len(salientWords): # check to make sure we can iterate over these together\n",
    "    for index, doc in enumerate(salientWords):\n",
    "        #print(\"Sentence # %d: \"%(index))\n",
    "        indexLs = maximumIndexLs[index]\n",
    "        if len(indexLs) ==3: \n",
    "            sentenceNum.append((index + 1))\n",
    "            for j in range(3):\n",
    "                tempWord = doc[(indexLs[j])]\n",
    "                #print(\"Word %d: %s\"%((j+1), tempWord))\n",
    "                if j == 0:\n",
    "                    wordOne.append(tempWord)\n",
    "                elif j == 1:\n",
    "                    wordTwo.append(tempWord)\n",
    "                elif j == 2:\n",
    "                    wordThree.append(tempWord)\n",
    "        else:\n",
    "            #print(\"Wrong number of maximums\")\n",
    "            someValue = 0\n",
    "else:\n",
    "    print(\"Not same length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNum</th>\n",
       "      <th>wordOne</th>\n",
       "      <th>wordTwo</th>\n",
       "      <th>wordThree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "      <td>denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>starte</td>\n",
       "      <td>day</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>guess</td>\n",
       "      <td>catching</td>\n",
       "      <td>beings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>please</td>\n",
       "      <td>kurt</td>\n",
       "      <td>vonnegut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>pneumonia</td>\n",
       "      <td>story</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>3096</td>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>compromise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>3097</td>\n",
       "      <td>ostracized</td>\n",
       "      <td>majority</td>\n",
       "      <td>compassion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>3098</td>\n",
       "      <td>receive</td>\n",
       "      <td>unshaved</td>\n",
       "      <td>brushing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>3100</td>\n",
       "      <td>lonesome</td>\n",
       "      <td>city</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2940</th>\n",
       "      <td>3101</td>\n",
       "      <td>understood</td>\n",
       "      <td>stayed</td>\n",
       "      <td>tank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2941 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentenceNum     wordOne   wordTwo   wordThree\n",
       "0               1      denver    denver      denver\n",
       "1               2      starte       day         day\n",
       "2               3       guess  catching      beings\n",
       "3               4      please      kurt    vonnegut\n",
       "4               5   pneumonia     story       story\n",
       "...           ...         ...       ...         ...\n",
       "2936         3096       could     could  compromise\n",
       "2937         3097  ostracized  majority  compassion\n",
       "2938         3098     receive  unshaved    brushing\n",
       "2939         3100    lonesome      city        city\n",
       "2940         3101  understood    stayed        tank\n",
       "\n",
       "[2941 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe with the top three words for each sentence\n",
    "topThreeWords = pd.DataFrame(zip(sentenceNum,wordOne,wordTwo,wordThree), columns = [\"sentenceNum\", \"wordOne\", \"wordTwo\", \"wordThree\"])\n",
    "topThreeWords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2109, 1015, 772, 2602, 2415, 2748, 1894, 373, 1290, 876]\n"
     ]
    }
   ],
   "source": [
    "# create a set of random pointers to filter out the word sets\n",
    "random.seed(120)\n",
    "pointers = []\n",
    "\n",
    "for i in range(10):\n",
    "    ptr = random.randint(0, len(sentenceNum))\n",
    "    pointers.append(ptr)\n",
    "    \n",
    "print(pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceNum</th>\n",
       "      <th>wordOne</th>\n",
       "      <th>wordTwo</th>\n",
       "      <th>wordThree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>1367</td>\n",
       "      <td>outdoorsey</td>\n",
       "      <td>carharrts</td>\n",
       "      <td>khuls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1368</td>\n",
       "      <td>instead</td>\n",
       "      <td>bozeman</td>\n",
       "      <td>vibe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1369</td>\n",
       "      <td>david</td>\n",
       "      <td>office</td>\n",
       "      <td>inform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentenceNum     wordOne    wordTwo wordThree\n",
       "1290         1367  outdoorsey  carharrts     khuls\n",
       "1291         1368     instead    bozeman      vibe\n",
       "1292         1369       david     office    inform"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out the df according to the pointer and the length of sentences. append each df to a list\n",
    "length = 3\n",
    "sets = []\n",
    "\n",
    "for index, ptr in enumerate(pointers):\n",
    "    cap = ptr + length # cap is the top index in DataFrame[ptr:cap]\n",
    "    tempDf = topThreeWords[ptr:cap]\n",
    "    sets.append(tempDf)\n",
    "    filePath = \"randomSamples/randomSet\" + str(index) + \".csv\"\n",
    "    tempDf.to_csv(filePath)\n",
    "     \n",
    "sets[8] # one is interestin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOriginalSentences(lsOfSent, lsOfPointers, lengthOfSet):\n",
    "    \n",
    "    sets = []\n",
    "\n",
    "    for index, ptr in enumerate(lsofPointers):\n",
    "        cap = ptr + length # cap is the top index in DataFrame[ptr:cap]\n",
    "        subSet = lsOfSent[ptr:cap]\n",
    "        string = \"\\n\".join(subSet)\n",
    "        print(string)\n",
    "        print(\"\")\n",
    "        #sets.append(subSet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "array(object, dtype=None, *, copy=True, order='K', subok=False, ndmin=0)\n",
       "\n",
       "Create an array.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "object : array_like\n",
       "    An array, any object exposing the array interface, an object whose\n",
       "    __array__ method returns an array, or any (nested) sequence.\n",
       "dtype : data-type, optional\n",
       "    The desired data-type for the array.  If not given, then the type will\n",
       "    be determined as the minimum type required to hold the objects in the\n",
       "    sequence.\n",
       "copy : bool, optional\n",
       "    If true (default), then the object is copied.  Otherwise, a copy will\n",
       "    only be made if __array__ returns a copy, if obj is a nested sequence,\n",
       "    or if a copy is needed to satisfy any of the other requirements\n",
       "    (`dtype`, `order`, etc.).\n",
       "order : {'K', 'A', 'C', 'F'}, optional\n",
       "    Specify the memory layout of the array. If object is not an array, the\n",
       "    newly created array will be in C order (row major) unless 'F' is\n",
       "    specified, in which case it will be in Fortran order (column major).\n",
       "    If object is an array the following holds.\n",
       "\n",
       "    ===== ========= ===================================================\n",
       "    order  no copy                     copy=True\n",
       "    ===== ========= ===================================================\n",
       "    'K'   unchanged F & C order preserved, otherwise most similar order\n",
       "    'A'   unchanged F order if input is F and not C, otherwise C order\n",
       "    'C'   C order   C order\n",
       "    'F'   F order   F order\n",
       "    ===== ========= ===================================================\n",
       "\n",
       "    When ``copy=False`` and a copy is made for other reasons, the result is\n",
       "    the same as if ``copy=True``, with some exceptions for `A`, see the\n",
       "    Notes section. The default order is 'K'.\n",
       "subok : bool, optional\n",
       "    If True, then sub-classes will be passed-through, otherwise\n",
       "    the returned array will be forced to be a base-class array (default).\n",
       "ndmin : int, optional\n",
       "    Specifies the minimum number of dimensions that the resulting\n",
       "    array should have.  Ones will be pre-pended to the shape as\n",
       "    needed to meet this requirement.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : ndarray\n",
       "    An array object satisfying the specified requirements.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "empty_like : Return an empty array with shape and type of input.\n",
       "ones_like : Return an array of ones with shape and type of input.\n",
       "zeros_like : Return an array of zeros with shape and type of input.\n",
       "full_like : Return a new array with shape of input filled with value.\n",
       "empty : Return a new uninitialized array.\n",
       "ones : Return a new array setting values to one.\n",
       "zeros : Return a new array setting values to zero.\n",
       "full : Return a new array of given shape filled with value.\n",
       "\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When order is 'A' and `object` is an array in neither 'C' nor 'F' order,\n",
       "and a copy is forced by a change in dtype, then the order of the result is\n",
       "not necessarily 'C' as expected. This is likely a bug.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.array([1, 2, 3])\n",
       "array([1, 2, 3])\n",
       "\n",
       "Upcasting:\n",
       "\n",
       ">>> np.array([1, 2, 3.0])\n",
       "array([ 1.,  2.,  3.])\n",
       "\n",
       "More than one dimension:\n",
       "\n",
       ">>> np.array([[1, 2], [3, 4]])\n",
       "array([[1, 2],\n",
       "       [3, 4]])\n",
       "\n",
       "Minimum dimensions 2:\n",
       "\n",
       ">>> np.array([1, 2, 3], ndmin=2)\n",
       "array([[1, 2, 3]])\n",
       "\n",
       "Type provided:\n",
       "\n",
       ">>> np.array([1, 2, 3], dtype=complex)\n",
       "array([ 1.+0.j,  2.+0.j,  3.+0.j])\n",
       "\n",
       "Data-type consisting of more than one element:\n",
       "\n",
       ">>> x = np.array([(1,2),(3,4)],dtype=[('a','<i4'),('b','<i4')])\n",
       ">>> x['a']\n",
       "array([1, 3])\n",
       "\n",
       "Creating an array from sub-classes:\n",
       "\n",
       ">>> np.array(np.mat('1 2; 3 4'))\n",
       "array([[1, 2],\n",
       "       [3, 4]])\n",
       "\n",
       ">>> np.array(np.mat('1 2; 3 4'), subok=True)\n",
       "matrix([[1, 2],\n",
       "        [3, 4]])\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentFrequency[\"van\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueDf(uniqueArr):\n",
    "    unique = pd.DataFrame(uniqueArr, columns=['token', 'indexFirstTokenOccur','frequency']) # convert to dataframe for easier wrangling\n",
    "    unique.frequency = unique.frequency.astype(int) # before this line was added, the count/frequency had at some point been converted to a string and so\n",
    "    # the sort values function was sorting it based upon the first value in a string (e.g. 1 comes before 9, so 1203 is smaller tahn 99)\n",
    "    unique = unique.sort_values(by='frequency', ascending = False)\n",
    "    #pring the head\n",
    "    \n",
    "    return(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueArr(lsOfAllWords):\n",
    "    \n",
    "    uniqueTokens, uniqueFirstOccurance, uniqueTokenCounts = np.unique(lsOfAllWords, return_index=True, return_counts=True) # how many unique words did I use in the writing of \n",
    "    # this book? what were the counts?\n",
    "    uniqueArr = np.asarray((uniqueTokens,uniqueFirstOccurance,uniqueTokenCounts)).T # turns tuple into ndarray then .T transposes it over its axis\n",
    "    \n",
    "    return(uniqueArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# although we already put this through a dict above, I would rather use lists\n",
    "allSalientWords = []\n",
    "\n",
    "for sentIndex, sent in enumerate(salientWords):\n",
    "    for wordIndex, word in enumerate(sent):\n",
    "        allSalientWords.append(word)\n",
    "\n",
    "len(allSalientWords)\n",
    "(\"van\" in allSalientWords) # so the word van is in salient words, so it got lost somewhere in the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "source": [
    "jackeroo = 0\n",
    "for i, e in enumerate(allSalientWords):\n",
    "    if e == \"van\":\n",
    "        jackeroo += 1\n",
    "        \n",
    "print(jackeroo) # so we have the same len of van instances as the other doc, so we know\n",
    "# SOMETHING BREAKS AFTER THIS CELL\n",
    "# probably has to do with the damn pos tag shit lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30970"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salientWordsPOS = nltk.pos_tag(allSalientWords) # add parts of speech (returns tuple)\n",
    "len(salientWordsPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0', 'CD'],\n",
       "       ['000', 'CD'],\n",
       "       ['000lbs', 'CD'],\n",
       "       ...,\n",
       "       ['zoomed', 'VBD'],\n",
       "       ['zucchini', 'CD'],\n",
       "       ['zzzzz', 'JJ']], dtype='<U17')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threeArrs = np.unique(salientWordsPOS, return_index=True, return_counts=True,axis = 0)\n",
    "wordPosArr = threeArrs[0]\n",
    "wordPosArr[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8606 8606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word, pos = zip(*wordPosArr)\n",
    "print(len(word), len(pos))\n",
    "\"van\" in word # so we still have van up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#allSalientWordsDf = pd.DataFrame(threeArrs)\n",
    "allSalientWordsDf = pd.DataFrame(zip(word, pos, threeArrs[1], threeArrs[2]), columns = [\"word\", \"pos\", \"index\", \"frequency\"])\n",
    "allSalientWordsDf.sort_values(by=\"frequency\", ascending = False)\n",
    "(\"van\" in allSalientWordsDf['word']) ### WHY!?!? WHY \"VAN\" OF ALL WORDS?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pos</th>\n",
       "      <th>index</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>like</td>\n",
       "      <td>IN</td>\n",
       "      <td>376</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6442</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>29</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7742</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "      <td>64</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5276</th>\n",
       "      <td>one</td>\n",
       "      <td>CD</td>\n",
       "      <td>32</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>would</td>\n",
       "      <td>MD</td>\n",
       "      <td>193</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>back</td>\n",
       "      <td>RB</td>\n",
       "      <td>262</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>road</td>\n",
       "      <td>NN</td>\n",
       "      <td>551</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>life</td>\n",
       "      <td>NN</td>\n",
       "      <td>127</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8128</th>\n",
       "      <td>van</td>\n",
       "      <td>NN</td>\n",
       "      <td>280</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>way</td>\n",
       "      <td>NN</td>\n",
       "      <td>49</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>home</td>\n",
       "      <td>NN</td>\n",
       "      <td>67</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>across</td>\n",
       "      <td>IN</td>\n",
       "      <td>163</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>could</td>\n",
       "      <td>MD</td>\n",
       "      <td>50</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>people</td>\n",
       "      <td>NNS</td>\n",
       "      <td>141</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108</th>\n",
       "      <td>night</td>\n",
       "      <td>NN</td>\n",
       "      <td>1295</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>still</td>\n",
       "      <td>RB</td>\n",
       "      <td>1363</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>day</td>\n",
       "      <td>NN</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>began</td>\n",
       "      <td>VBD</td>\n",
       "      <td>200</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>went</td>\n",
       "      <td>VBD</td>\n",
       "      <td>1125</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>coffee</td>\n",
       "      <td>NN</td>\n",
       "      <td>328</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  pos  index  frequency\n",
       "4439    like   IN    376        205\n",
       "6442    said  VBD     29        158\n",
       "7742    time   NN     64        157\n",
       "5276     one   CD     32        144\n",
       "8499   would   MD    193        137\n",
       "533     back   RB    262        130\n",
       "6324    road   NN    551        129\n",
       "4418    life   NN    127        121\n",
       "8128     van   NN    280        115\n",
       "8303     way   NN     49        113\n",
       "3735    home   NN     67        113\n",
       "140   across   IN    163        109\n",
       "1770   could   MD     50        108\n",
       "5543  people  NNS    141        100\n",
       "5108   night   NN   1295         96\n",
       "7276   still   RB   1363         89\n",
       "2029     day   NN     19         86\n",
       "684    began  VBD    200         83\n",
       "8334    went  VBD   1125         82\n",
       "1513  coffee   NN    328         82"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSalientWordsDf.sort_values(by=\"frequency\", ascending = False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 3, 1, 1])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
